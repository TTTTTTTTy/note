{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit784e7719b6ff4019a0ac1f2c475e820e",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch常用代价函数示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'1.6.0'"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 交叉熵\n",
    "交叉熵的公式为： $H(p,q)=-\\sum_{i}\\ P(i)logQ(i)$  其中P为真实值，Q为预测值。\n",
    "\n",
    "当label为确定的类时，P是一个one-hot向量，交叉熵就相当于 $H(p,q)=-logQ(m)$ 其中m是正确类别下标\n",
    "\n",
    "#### nn.CrossEntropyLoss() / F.cross_entropy()\n",
    "计算过程：对outputs每一个样本进行softmax，再计算交叉熵，最后求平均\n",
    "\n",
    "参数：\n",
    "\n",
    "`torch.nn.CrossEntropyLoss(weight: Optional = None, ignore_index: int = -100, reduction: str = 'mean')`\n",
    "\n",
    "`weight(Tensor)`- 为每个类别的loss设置权值，常用于类别不均衡问题。weight必须是float类型的tensor，其长度要与类别C一致，即每一个类别都要设置有weight\n",
    "\n",
    "`ignore_index(int)`- 忽略某一类别，不计算其loss，其loss会为0,并且，在reduction='mean'或'sum'时，不会计算那一类的loss，除的时候的分母也不会统计那一类的样本。\n",
    "\n",
    "`reduction(str)`- 默认为'mean'， 设置为'none'时，返回所有样本的loss；设置为'mean'时，返回所有样本loss的平均值；设置为'sum'时，返回所有样本loss的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "use nn.CrossEntropyLoss(): tensor(1.0424)\nuse F.cross_entropy(): tensor(1.0424)\nno api: tensor(1.0424)\n"
    }
   ],
   "source": [
    "'''\n",
    "示例1：nn.CrossEntropyLoss() / F.cross_entropy()基本用法\n",
    "'''\n",
    "outputs = torch.randn(3, 5) # (batch_size, class_num)\n",
    "labels = torch.tensor([0, 2, 4]) # (batch_size, )\n",
    "# nn.CrossEntropyLoss()\n",
    "loss_fun = nn.CrossEntropyLoss() \n",
    "print('use nn.CrossEntropyLoss():', loss_fun(outputs, labels))\n",
    "# F.cross_entropy()\n",
    "print('use F.cross_entropy():',F.cross_entropy(outputs, labels))\n",
    "# 等同于下面的计算过程\n",
    "loss = 0.\n",
    "probs = F.log_softmax(outputs, dim=1)  # softmax后取log\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i].item()\n",
    "    loss += -probs[i][label] # 正确类别的-log(p)相加\n",
    "loss /= len(labels) # 取平均\n",
    "print('no api:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "reduction='none': tensor([0.8890, 0.7958, 0.0000])\nreduction='mean': tensor(0.8424)\nreduction='sum': tensor(1.6848)\n"
    }
   ],
   "source": [
    "'''\n",
    "示例2：nn.CrossEntropyLoss() / F.cross_entropy()参数设置\n",
    "'''\n",
    "print(\"reduction='none':\", F.cross_entropy(outputs, labels, reduction='none', ignore_index=4))\n",
    "print(\"reduction='mean':\", F.cross_entropy(outputs, labels, reduction='mean', ignore_index=4))\n",
    "print(\"reduction='sum':\", F.cross_entropy(outputs, labels, reduction='sum', ignore_index=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.NLLLoss() / F.nll_loss()\n",
    "计算过程：对probs中正确类别的prob相加取平均, 所以F.cross_entropy() = F.log_softmax() + F.nll_loss()\n",
    "\n",
    "参数：\n",
    "\n",
    "`torch.nn.NLLLoss(weight: Optional = None, ignore_index: int = -100, reduction: str = 'mean')`\n",
    "\n",
    "参数定义同CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(1.0424)\ntensor(1.0424)\n"
    }
   ],
   "source": [
    "'''\n",
    "示例3：nn.NLLLoss() / F.nll_loss()基本用法\n",
    "'''\n",
    "probs = F.log_softmax(outputs, dim=1)  # probs是outputs取log_softmax的结果\n",
    "# nn.NLLLoss()\n",
    "loss = nn.NLLLoss()\n",
    "print(loss(probs,labels))\n",
    "# F.nll_loss()\n",
    "print(F.nll_loss(probs, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 均方误差\n",
    "均方误差的公式为： $loss(y,y')=\\frac{1}{n}\\sum(y-y')^2$ \n",
    "\n",
    "#### nn.MSELoss() / F.mse_loss()\n",
    "\n",
    "参数：\n",
    "\n",
    "`torch.nn.MSELoss(reduction: str = 'mean')`\n",
    "\n",
    "`reduction(str)`- 默认为'mean'， 设置为'none'时，返回所有样本的loss；设置为'mean'时，返回所有样本loss的平均值；设置为'sum'时，返回所有样本loss的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([-0.6892,  0.2099,  1.0249, -1.4212,  0.0505])\ntensor([ 1.6486, -1.1726, -0.3007, -0.7615,  1.7021])\ntensor(2.4594)\ntensor(2.4594)\ntensor(2.4594)\n"
    }
   ],
   "source": [
    "'''\n",
    "示例4：nn.MSELoss() / F.mse_loss()基本用法\n",
    "'''\n",
    "# 实际值:采样自标准正态分布N~(0,1)的5个值\n",
    "y = torch.randn(5)\n",
    "# 预测值\n",
    "pred = torch.randn(5)\n",
    "print(y)\n",
    "print(pred)\n",
    "\n",
    "# nn.MSELoss()\n",
    "loss = nn.MSELoss()\n",
    "print(loss(pred, y))\n",
    "# F.mse_loss()\n",
    "print(F.mse_loss(pred, y))\n",
    "# 模拟计算过程\n",
    "mse = (pred - y).pow(2).mean() \n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KL散度（相对熵）\n",
    "KL散度的公式为： $D_{KL} (p||q)=H(p,q)-H(p)=\\sum_x p(x)log\\frac{p(x)}{q(x)}$ \n",
    "\n",
    "其中p表示真实分布，q表示p的拟合分布，D(P||Q)表示当用概率分布q来拟合真实分布p时，产生的信息损耗。\n",
    "\n",
    "#### nn.KLDivLoss() /F.kl_div()\n",
    "\n",
    "参数：\n",
    "\n",
    "`torch.nn.KLDivLoss(reduction: str = 'mean', log_target: bool = False)`\n",
    "\n",
    "`reduction(str)`- 可选 none||sum||mean||batchmean，默认为'mean'，设置为'none'时，不做reduction操作；设置为'mean'时，返回output的均值；设置为'sum'时，返回output的和; 设置为batchmean时，返回总和除以batch_size的值\n",
    "\n",
    "***一般来说，输入为单个概率分布时，reduction设置为sum，输入为多个样本的概率分布时，reduction设置为batchmean***\n",
    "\n",
    "`log_target(bool)`[1.6.0引入]- 默认为False，指明是否target在传入时是否已经取log\n",
    "\n",
    "注意！！！：input在传入时需要取log，如果log_target没有被设置为True，target在传入时不需要取log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(0.4322)\ntensor(0.4322)\ntensor(0.4322)\ntensor(0.4322)\n"
    }
   ],
   "source": [
    "'''\n",
    "示例5：nn.KLDivLoss() /F.kl_div()基本用法\n",
    "'''\n",
    "target = torch.FloatTensor([0.1, 0.2, 0.7])\n",
    "pred = torch.FloatTensor([0.5, 0.2, 0.3])\n",
    "# nn.KLDivLoss\n",
    "loss_fun = nn.KLDivLoss(reduction='sum') \n",
    "print(loss_fun(pred.log(), target))\n",
    "# F.kl_div()\n",
    "print(F.kl_div(pred.log(), target, reduction='sum'))\n",
    "print(F.kl_div(pred.log(), target.log(), reduction='sum', log_target=True)) # 等价\n",
    "\n",
    "#上面的计算过程等价于下面\n",
    "loss = (target * ((target/pred).log())).sum()\n",
    "print(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.1609,  0.0000,  0.5931],\n        [ 0.0000,  0.1151, -0.0863]])\ntensor(0.4609)\ntensor(0.0768)\ntensor(0.2305)\ntensor(0.2305)\n"
    }
   ],
   "source": [
    "'''\n",
    "示例6：nn.KLDivLoss() /F.kl_div()参数reduction设置\n",
    "'''\n",
    "target = torch.FloatTensor(([0.1, 0.2, 0.7], [0.3, 0.4, 0.3]))\n",
    "pred = torch.FloatTensor(([0.5, 0.2, 0.3], [0.3, 0.3, 0.4]))\n",
    "# F.kl_div()\n",
    "print(F.kl_div(pred.log(), target, reduction='none'))\n",
    "print(F.kl_div(pred.log(), target, reduction='sum'))\n",
    "print(F.kl_div(pred.log(), target, reduction='mean'))\n",
    "print(F.kl_div(pred.log(), target, reduction='batchmean'))\n",
    "\n",
    "#batchmean相当于对每个概率分布的kl散度求平均（符合数学上的定义）\n",
    "loss = (target * ((target/pred).log())).sum(dim=1).mean()\n",
    "print(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}